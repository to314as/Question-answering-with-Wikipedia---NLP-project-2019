{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism', 'Autism', 'Albedo', 'A', 'Alabama', 'Achilles']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-12567698f355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0marticle_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' title=\"(.*)\">'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'>'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0marticle_title\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'(.*)</doc>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0marticle_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import bz2\n",
    "\n",
    "dataset = []\n",
    "# iterate to all the folders and get the title and file name from all the index.html files\n",
    "\n",
    "# read the file from index files\n",
    "file = open(os.getcwd()+\"/wiki_00.txt\", 'r')\n",
    "file = bz2.open(\"wiki_00.bz2\", \"rb\")\n",
    "text = file.read().strip().decode(\"utf-8\") \n",
    "#print(text)\n",
    "file.close()\n",
    "\n",
    "# use simple regular expression to retrieve the article id, url, title and body.\n",
    "article_id = re.findall('<doc id=\"(.*)\" ', text)\n",
    "article_url = re.findall(' url=\"(.*)\" ', text)\n",
    "article_title = re.findall(' title=\"(.*)\">', text)\n",
    "print(article_title)\n",
    "regex = '>'+article_title+'(.*)</doc>'\n",
    "article_body = re.findall(regex, text)\n",
    "\n",
    "print(len(article_id),len(article_url),len(article_title),len(article_body))\n",
    "\n",
    "for j in range(len(article_id)):\n",
    "    dataset.append(article_id,article_url,article_title,article_body)     \n",
    "        \n",
    "N = len(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    # iterate over all the stop words and not append to the list if it’s a stop word\n",
    "    new_text = \"\"\n",
    "    for word in words:\n",
    "        if word not in stop_words and len(word) > 1: # remove stop words and single characters\n",
    "            new_text = new_text + \" \" + word\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ') # remove every occurence of this symbol\n",
    "        data = np.char.replace(data, \"  \", \" \") # remove extra spaces\n",
    "    data = np.char.replace(data, ',', '') #remove comma seperately at last?\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\") #seperate?\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(data): # reduce words to its stem\n",
    "    stemmer= PorterStemmer() # rule-based stemmer, identifies and removes the suffix or affix of a word\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "# A better efficient way to proceed is to first lemmatise and then stem\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w),lang='en')\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "for i in dataset:\n",
    "    processed_text.append(word_tokenize(str(preprocess(i[3]))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "# iterate through all the words in all the documents and store the document id’s for each word.\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i] # body of the document\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i]) # unique words, we don’t actually need the list of docs, we just need the count\n",
    "\n",
    "total_vocab_size = len(DF)\n",
    "total_vocab = [x for x in DF]\n",
    "\n",
    "#getter\n",
    "def doc_freq(word):\n",
    "    try:\n",
    "        return DF[word]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s use dictionary with (document, token) pair as key and any TF-IDF score as the value\n",
    "# tf_idf dictionary is for body, we will use the same logic for to build a dictionary tf_idf_title for the words in title.\n",
    "from collections import Counter\n",
    "\n",
    "# Calculate TF-IDF for Body for all docs\n",
    "doc = 0\n",
    "tf_idf = {}\n",
    "#iterate over all documents\n",
    "for i in range(N):  \n",
    "    tokens = processed_text[i]\n",
    "    counter = Counter(tokens + processed_title[i])  \n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    # Counter can give us the frequency of the tokens, calculate tf and idf and finally store as a (doc, token) pair in tf_idf.\n",
    "    for token in np.unique(tokens):      \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
    "        tf_idf[doc, token] = tf*idf\n",
    "    doc += 1\n",
    "\n",
    "# Calculate TF-IDF for title for all docs   \n",
    "doc = 0\n",
    "tf_idf_title = {}\n",
    "for i in range(N):\n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_text[i])\n",
    "    words_count = len(tokens + processed_text[i])\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "# Merging the TF-IDF according to weights\n",
    "# multiply the Body TF-IDF with alpha\n",
    "for i in tf_idf:\n",
    "    tf_idf[i] *= alpha\n",
    "# Iterate Title IF-IDF for every (doc, token)\n",
    "# if token is in body, replace the Body(doc, token) value with the value in Title(doc, token)\n",
    "for i in tf_idf_title:\n",
    "    tf_idf[i] = tf_idf_title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize documents\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1]) # generate a index for each token\n",
    "        D[i[0]][ind] = tf_idf[i] # document vectors\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical concept: add tf_idf values of the tokens that are in query for every document.\n",
    "# Iterate over all values in the dictionary and check if the value is present in the token.\n",
    "# As our dictionary is a (document, token) key, when we find a token which is in query we will\n",
    "# add the document id to another dictionary along with the tf-idf value\n",
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "    for key in tf_idf:  \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    print(query_weights)\n",
    "    # take the top k documents\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\")\n",
    "    ranking = []\n",
    "    for i in query_weights[:k]:\n",
    "        print(i)\n",
    "        ranking.append(i[0])\n",
    "    print(ranking)\n",
    "    for i in ranking:\n",
    "        print(i, dataset[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# vectorize query\n",
    "def gen_vector(tokens):\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    query_weights = {}\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q\n",
    "\n",
    "def cosine_similarity(k, query):\n",
    "    print(\">>Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(tokens)   \n",
    "    query_vector = gen_vector(tokens)\n",
    "    d_cosines = []\n",
    "    for d in D:\n",
    "        s = np.dot(query_vector, d)/(np.linalg.norm(query_vector)*np.linalg.norm(d))\n",
    "        d_cosines.append(s)\n",
    "    # take the top k documents\n",
    "    ranking = np.array(d_cosines).argsort()[-k:][::-1].tolist()  \n",
    "    print(\"\") \n",
    "    print(ranking)\n",
    "    for i in ranking:\n",
    "        print(i, corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is unsupervised learning?\"\n",
    "matching_score(10,query)\n",
    "print(\"\")\n",
    "cosine_similarity(10,query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
